# -*- coding: utf-8 -*-
"""clean_and_transform_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16d_3xltLh2DJNT-FsYz6_OJRBWTY_qOC

code adapted from:

https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/
"""

# Commented out IPython magic to ensure Python compatibility.
import sys
# !{sys.executable} -m spacy download en
import re, numpy as np, pandas as pd
from pprint import pprint

# Gensim
import gensim, spacy, logging, warnings
import gensim.corpora as corpora
from gensim.utils import lemmatize, simple_preprocess
from gensim.models import CoherenceModel
import matplotlib.pyplot as plt
# NLTK Stop words
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
stop_words = stopwords.words('english')
stop_words.extend(['deleted', 'removed', 'from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])

# %matplotlib inline
warnings.filterwarnings("ignore",category=DeprecationWarning)
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)

from google.colab import drive
drive.mount('/content/gdrive')

# !python3 -m spacy download en  # run in terminal once
def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """Remove Stopwords, Form Bigrams, Trigrams and Lemmatization"""
    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
    texts = [bigram_mod[doc] for doc in texts]
    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]
    texts_out = []
    nlp = spacy.load('en', disable=['parser', 'ner'])
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    # remove stopwords once more after lemmatization
    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    
    return texts_out

# https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92
from google.colab import files
#uploaded = files.upload()

import csv
def sent_to_words(sentences, typeRow):      
    # open CSV
      for sent in sentences:
          
          if(isinstance(sent, str)):

            # remove URLs

            # remove repetition (indented part that is reply to a part of another comment)

            # remove all non alphabet characters
            sent = ''.join([i if ((ord(i) > 64 and ord(i) < 91) or 
                                  (ord(i) > 96 and ord(i) < 123) or
                                  ord(i) == 32) 
                            else '' for i in sent])
            

            # tokenize
            sent = gensim.utils.simple_preprocess(str(sent), deacc=True) 
            yield(sent)  
    

# data_words to text file
#print(data_words[:1])
# [['from', 'irwin', 'arnstein', 'subject', 're', 'recommendation', 'on', 'duc', 'summary', 'whats', 'it', 'worth', 'distribution', 'usa', 'expires', 'sat', 'may', 'gmt', ...trucated...]]

subreddits_and_posts = [('SanJose', ['dd9z5i', 'hctkmi', 'tc9zya', 'qpmced', 'nikckh', 'tei5j2', 't656c2', 's4vrfh', 'ri35yo', 'd30q6i', 'pfgihw', 'orf7mw', '6cwdns', 'c13can', 'jiqt10', 'q173cs', 'th0w9l', '5w875m']),
                         ('Chicago', ['gc9a46', 'tr5gma', '5g14s5', 'd4ly14', 'bw513v', 'acx2uk', 'o8luqs', 'ihzgzs', 'puv9ho', '8wtufi', '2czety', '6rmhvm', 'tz57az', '5tft23', 'nnrgix', '8xgyrf', 'q6pohw', 'b1xud9', '36bliu', 'w8o1u', '5dixdk', 'qzly17', 'ok945q', '4a25uh', 'ndp5la', 'ti4vst', 'bxzfgc', '']),
                        ('SanFrancisco', ['k7c7ap', 'mfdm67', 't9kwtv', 'i845mi', 'e00b9i', 'drnzvb', 'raivqd', 'dptnqc', '8ljl0b', 'rcpgzy', 'puwwwx', 's3edlh', 'gbr7l8', 'qo7ad4', 'g5bve1', 'rabs27', 'eeezu0', 'ovkd2u', '64jm7y', 'pb2myl', '93in66', 'enbmpd', 'u5v5dy', 'ss65lu']),
                        ('Houston', ['6wgeyy', 'r16axy', 'crsl1t', 'orksyk', 'syquc0', 'ri37nx', 'oehkrp', '73wdxv', '6zbz6j', 'wtjzj', 'so5lby', 'r6l424', '4r8rgw', '1rjum8', 'einn1k', '5l9q03']),
                        ('NYC', ['cprd2g', 'd2oiy5', 'b93v3s', 'eqilh0', 'iclplf', 'tw3j3v', 'hg6usx', 'iasa6e', 'myzmkn', '2blscl', 'b43b9p', 'jfp2dr']),
                        ('Phoenix', ['g3hgdv', 'gl0vo0', 'cimp33', 'divj2s', 'dwx78x', 'hd7d3s', 'g1ktq9', '90w1op', 'b0bbx4', 's7snn2', '84dwiu', 'met40l', 'rc7rev', '89a2yd', '3l6c52', 'p42rda', '4o8k4d', 's11njr', '90hyzq', 'hie5wp', 'm2b443', 'ee19ms']),
                        ('LosAngeles', ['pcprcq', 'cu1gff', '5qe28g', 'i1tjd9', '23u580', 'awiuwj', 'rkfymp', '76w8k4', 'rwi09a', 'g9phbl', '505ixk', '7rqrn1', 'tx2tf5', '1ip01b', '65xhae', '19g88g', 'i9dyr2', 'qqgag0']),
                        ('Philadelphia', ['lk3j4c', 'jsgf3m', 'kwb7kc', '8me9qz', 'ovrjkz', 'mp3kn4', '775pw8', 'smsvtm', '3vo57k', '1wag9q']),
                        ('Dallas', ['i4vnta', 'ki9mig', 'ng6fkl', 'qqcjti', 'tq3s0q', 'r6vz0y', 'qu3bc7', 'quqza1', 'snul9j', 'rmj5dg', 's2cgd4', 'rtxwxs', '25djly', 'tb6pgq', 'rpcu3g', 'tyrls5', 'daq61b', 'ru1pos', '36ibgp', '2qo42t',]),
                        ('SanDiego', ['kenvou', 'oenpon', 'illrcr', 't8t53i', 'radh95', 'mr4t4f', 'sflqqk', 't9us2t', 'rcguqb', 'u5wxm1', 'c593y0', 'tc6eae', 'm5cjqu', '87sc9y', '1vyfa1', 'hm01pl']),
                        ('SanAntonio', ['kz7u7b', 'k5jggx', '6jwmsq', 'onfcrd', 'khjril', 'a6pzty', '69l28u', 'mjpkwk', '228523', 'ubbwga', 'mmw4yk', '91v00j', '86ggwu', '83g7ne', 'saulm2'])
]

for sp in subreddits_and_posts: 
  filename, relevantPosts = sp
  col_list = ["comment_body", "comment_parent_id"]
  df = pd.read_csv('/content/gdrive/My Drive/AllComments/'+ filename + '_comments_Movingsubreddit.csv', usecols=col_list)    
  df['comment_parent_id'] = df['comment_parent_id'].str[3:]
  print(df.count())
  df = df.loc[df['comment_parent_id'].isin(relevantPosts)]
  print(df.count())

  train, validate, test = \
              np.split(df.sample(frac=1, random_state=42), 
                       [int(.8*len(df)), int(.9*len(df))])
              

  #with open((filename +'.tsv'), 'w', newline='') as f_output:
  data = train['comment_body'].values.tolist()
  train_words = list(sent_to_words(data, "train"))

  data = test['comment_body'].values.tolist()
  test_words = list(sent_to_words(data, "test"))

  data = validate['comment_body'].values.tolist()
  validate_words = list(sent_to_words(data, "val"))

  data = df['comment_body'].values.tolist()
  data_words = list(sent_to_words(data, "no"))


  # Build the bigram and trigram models
  bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
  trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  
  bigram_mod = gensim.models.phrases.Phraser(bigram)
  trigram_mod = gensim.models.phrases.Phraser(trigram)

  data_ready = process_words(data_words)  # processed Text Data!
  train_ready = process_words(train_words)  
  val_ready = process_words(validate_words)  
  test_ready = process_words(test_words)
              

  # Create Dictionary
  id2word = corpora.Dictionary(data_ready)

  textfile = open((filename + ".txt"), "w")
  for pair in id2word.iteritems():
      _, word = pair
      textfile.write(word + "\n")
  textfile.close()

  with open((filename +'.tsv'), 'w', newline='') as f_output:
      tsv_output = csv.writer(f_output, delimiter='\t')

      for row in train_ready:
        s =""
        for elem in row:
          s+= elem + " "
        if(s):
          tsv_output.writerow([s, "train"])
        
      for row in test_ready:
        s =""
        for elem in row:
          s+= elem + " "
        if(s):
          tsv_output.writerow([s, "test"])
      for row in val_ready:
        s =""
        for elem in row:
          s+= elem + " "
        if(s):
          tsv_output.writerow([s, "val"])

  files.download(filename+".tsv")
  files.download(filename+".txt")

#import io
##import pandas as pd
#col_list = ["comment_body", "comment_parent_id"]
#df = pd.read_csv(io.BytesIO(uploaded[('/content/gdrive/AllComments/'+ filename + '_comments_Movingsubreddit.csv')]), usecols=col_list)

# Dataset is now stored in a Pandas Dataframe